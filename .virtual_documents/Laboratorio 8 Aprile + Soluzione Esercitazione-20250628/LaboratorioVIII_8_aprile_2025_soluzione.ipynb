








import numpy as np
import numpy.linalg as npl
import scipy.linalg as spl
import matplotlib.pyplot as plt


#Calcolo della norma infinito
def mynorma_inf(B):
    c=np.sum(np.abs(B),axis=1)  #equivale a sommare gli elementi di tutte le colonne
    #(che equivale a fare la somma degli elementi di ogni riga)
    n_inf=np.max(c)
    return n_inf


x=np.arange(1.0,7.0)
A=np.vander(x, increasing=True)
print(A)
mycond=mynorma_inf(A)*mynorma_inf(np.linalg.inv(A))
print("mycond {:e}".format(mycond))
condp=np.linalg.cond(A,np.inf)
print("condizionamento in norma infinito con numpy {:e}".format(condp))


#Costruisco il termine noto in maniera tale che la soluzione sia il vettore unitario
b=np.sum(A,axis=1).reshape(6,1) #Calcolo la somma degli elementi di ogni riga 
#Con la scelta di questo termine noto la soluzione esatta è il vettore di tutti 1.
print(b)


bp=b.copy()
bp[0]=bp[0]+0.025  #Perturbo l'elemento di posizione 0 del termine noto di una quantità pari a 0.025
errore_dati=np.linalg.norm(bp-b,np.inf)/np.linalg.norm(b,np.inf)
print("errore dati ",errore_dati)


xesatta= np.ones((6,1))  #Soluzione esatta
xp=spl.solve(A,bp)
print("Soluzione esatta sistema senza perturbazione \n ",xesatta)
print("Soluzione sistema con termine noto perturbato\n ",xp)
errore_soluzione=np.linalg.norm(xp-xesatta,np.inf)/np.linalg.norm(xesatta,np.inf)
print("errore soluzione ",errore_soluzione)
#Ad un errore relativo sui dati pari a 2.67 e-06, corrisponde un errore relativo sulla soluzione di 0.2175.
#Questo dipende dal fatto che il problema della soluzione del sistema lineare con la matrice A come nell'esercizio 
#è mal condizionato, quindi piccole perturbazioni sui dati vengono amplificate sulla soluzione
#l'elevato indice di condizionamento della matrice A, che abbiamo visto essere 1.281105e+06 conferma la precedente.
#Infatti abbiamo detto a lezione che l'indice di condizionamento della matrice A rappresenta un fattore di amplificazione
#sulla soluzione di piccoli errori sui dati





#Esempi di sistemi malcondizionati


A=np.array([[6, 63, 662.2],[63, 662.2, 6967.8],[662.2, 6967.8, 73393.5664]])
b=np.array([1.1, 2.33, 1.7])

KA= npl.cond(A,np.inf)
print("Indice di condizionamento di A {:e}".format(KA))
x=spl.solve(A,b)

#perturbare la matrice
 
A1=A.copy()
A1[0,0]=A[0,0]+0.01
x_per=spl.solve(A1,b)


#Errore relativo sui dati
err_dati=npl.norm(A-A1,np.inf)/npl.norm(A,np.inf)

print("Errore relativo sui dati ", err_dati)
print("Errore relativo sui dati  in percentuale ", err_dati*100,"%")

err_rel_sol=npl.norm(x_per-x,np.inf)/npl.norm(x,np.inf)
print("Errore relativo sulla soluzione ", err_rel_sol)
print("Errore relativo sulla soluzione  in percentuale ", err_rel_sol*100,"%")
#Ad un errore relativo sui dati pari a 1.23 e-07, corrisponde un errore relativo sulla soluzione di 0.9995.
#Questo dipende dal fatto che il problema della soluzione del sistema lineare con la matrice A come nell'esercizio 
#è mal condizionato, quindi piccole perturbazioni sui dati vengono amplificate sulla soluzione
#l'elevato indice di condizionamento della matrice A, che abbiamo visto essere 1.975302e+010 conferma la precedente affermazione.
#Infatti abbiamo detto a lezione che l'indice di condizionamento della matrice A rappresenta un fattore di amplificazione
#sulla soluzione di piccoli errori sui dati





n=4
A=spl.hilbert(4)
print(A)
condA=np.linalg.cond(A,np.inf)
print("Indice di condizionamento di A {:e}".format(condA))

b=np.array([1,1,1,1])

x= spl.solve(A,b)


db=np.array([0.01, -0.01, 0.01, -0.01])
bp=b+db

xp=spl.solve(A,bp)

err_dati= npl.norm(db,2)/npl.norm(b,2)  
print("Errore relativo sui dati ", err_dati)
print("Errore relativo sui dati  in percentuale", err_dati*100,"%")

err_rel_sol=npl.norm(x-xp,np.inf)/npl.norm(x,np.inf) 
print("Errore relativo sulla soluzione", err_rel_sol)
print("Errore relativo sulla soluzione  in percentuale", err_rel_sol*100,"%")
#Ad un errore relativo sui dati pari a 0.01 (che in percentuale è pari all' 1%) , corrisponde un errore relativo 
#sulla soluzione di 0.7566 (che in percentuale è pari al 75%).
#Questo dipende dal fatto che il problema della soluzione del sistema lineare con la matrice A come nell'esercizio 
#è mal condizionato, quindi piccole perturbazioni sui dati vengono amplificate sulla soluzione
#l'elevato indice di condizionamento della matrice A, che abbiamo visto essere 2.837500e+04 conferma la precedente affermazione.
#Infatti abbiamo detto a lezione che l'indice di condizionamento della matrice A rappresenta un fattore di amplificazione
#sulla soluzione di piccoli errori sui dati








import numpy as np
import scipy as sp
from scipy.linalg import lu
A=np.array([[2,1],[3,4]])
PT,L,U=lu(A)  #Restituisce in output la trasposta della matrice di Permutazione
P=PT.copy()   #P è la matrice di permutazione
print("A=",A)
print("L=",L)
print("U=",U)
print("P=",P)
#LU è la fattorizzazione di P*A (terorema 2)
A1=P@A # equivale al prodotto matrice x matrice np.dot(P,A)
A1Fatt=L@U # equivale a np.dot(L,U)
print("Matrice P*A \n", A1)
print("Matrice ottenuta moltipicando Le ed U \n",A1Fatt)






from scipy.linalg import cholesky
A=np.array([[2,1,3],[1,5,7],[3,7,12]])
print(A)


L=cholesky(A,lower=True)
print(L)
A1=L@L.T
print("A1=\n",A1)





from scipy.linalg import qr
A=np.array([[2,1,3],[1,5,7],[3,7,12]])
Q,R=qr(A)
print("Q=",Q)
print("R=",R)
A1=Q@R
print(A1)





def LUsolve(P,A,L,U,b):
    pb=np.dot(P,b)
    y,flag=Lsolve(L,pb)
    if flag == 0:
         x,flag=Usolve(U,y)
    else:
        return [],flag

    return x,flag
        


import numpy as np
from SolveTriangular import *
from scipy.linalg import lu

A = np.array([[2, 5, 8, 7], [5, 2, 2, 8], [7, 5, 6, 6], [5, 4, 4, 8]])
print("A=",A)
b=np.sum(A,axis=1).reshape(4,1)
print("b=",b)
PT, L, U = lu(A)
P=PT.T.copy()
print("P= \n",P)
print("L=\n",L)
print("U=\n",U)
#Le permutazioni di righe fatte sulla matrice vengono effettuate anche sul termine noto
x,flag=LUsolve(P,A,L,U,b)
print("flag= \n", flag, "\n x= \n",x)





def solve_nsis(A,B):
  # Test dimensione  
    m,n=A.shape
    flag=0;
    if n!=m:
      print("Matrice non quadrata")
       
      return
    
     
    X= np.zeros((n,n))
    PT,L,U= lu(A)
    P=PT.T.copy()
    if flag==0:
        for i in range(n):
            y,flag=Lsolve(L,P@B[:,i])
            x,flag= Usolve(U,y)
            X[:,i]=x.reshape(n,)
    else:
        print("Elemento diagonale nullo")
        X=[]
    return X    
    


A_1=np.array([[3,5,7],[2,3,4],[5,9,11]])
m,n=A_1.shape
B=np.eye(m) #Si da come matrice B l'identità: così si ottiene per X l'inversa
X=solve_nsis(A_1,B) 
print("my inversa \n",X)
print("Inversa con linalg \n ",np.linalg.inv(A_1))

A_2=np.array([[1, 2, 3, 4], [2, -4, 6, 8],[-1, -2, -3, -1],[ 5, 7, 0 ,1]])
m,n=A_2.shape
B=np.eye(m)
X=solve_nsis(A_2,B) 
print("my inversa \n",X)
print("Inversa con linalg \n ",np.linalg.inv(A_2))  





PT,L_1,U_1=lu(A_2)
P=PT.copy()
deterA2=np.prod(np.diag(U_1))*np.linalg.det(P)
#In realtà bisognerebbe risalire al numero di scambi effettuati s dall'algoritmo di Gauss con pivoting e calcolare det(P)=(-1)**s,
#ma mi interessa solo sapere il risultato teorico e quindi potete tranquillamente usare la funzione np.linalg.det per calcolare det(P)
print("determinante sfruttando fattorizzazione LU",deterA2,"determinante sfruttando la funzione np.linalg.det ", np.linalg.det(A_2))





def Hankel(n):
    A=np.zeros((n,n),dtype=float)
    for i in range(0,n):
        for k in range(i+1-n,i+1):
            if k>0:
                A[i,n-1+k-i]=2.0**(k+1)
            else:
                A[i,n-1+k-i]=2.0**(1/(2-k-1))
    return A


indCond=[]
err_rel=[]
err_rel_qr=[]
for n in range(4,41,6):

   A=Hankel(n)
   indCond.append(np.linalg.cond(A,2))
   xesatta=np.ones((n,1))
   b=np.dot(A,xesatta)
   PT,L,U=lu(A)
   P=PT.T
   x,flag=LUsolve(P,A,L,U,b)
    
   err_rel.append(np.linalg.norm(x-xesatta,2)/np.linalg.norm(xesatta,2))
    
   
   Q,R=sp.linalg.qr(A)
   
   y=np.dot(Q.T,b)
   xqr,flag=Usolve(R,y)
   err_rel_qr.append(np.linalg.norm(xqr-xesatta,2)/np.linalg.norm(xesatta,2))
   
   
plt.semilogy(range(4,41,6),err_rel,'ro-',range(4,41,6),err_rel_qr,'go-')
plt.legend(['Gauss','QR'])
plt.ylabel('Errore relativo sulla soluzione')
plt.show()

#Si noti dall'andamento dell'errore relativo sulla soluzione,come spiegato in teoria, che il metodo QR è più stabile dell'algoritmo di Gauss 
#con pivotaggio








def matrix(n):
    A=np.zeros((n,n),dtype=float)
    for i in range(0,n):
        for j in range(0,n):
            if i==j or j==n-1:
                A[i,j]=1
            elif i>j:
                A[i,j]=-1
            else:
                A[i,j]=0
    return A


indCond=[]
err_rel=[]
err_rel_qr=[]
for n in range(48,59,2):

   A=matrix(n)
   indCond.append(np.linalg.cond(A,2))
   xesatta=np.ones((n,1))
   b=np.dot(A,xesatta)
   PT,L,U=lu(A)
   P=PT.T
   x,flag=LUsolve(P,A,L,U,b)
    
   err_rel.append(np.linalg.norm(x-xesatta,2)/np.linalg.norm(xesatta,2))
    
   
   Q,R=sp.linalg.qr(A)
   #Poichè Q è ortogonale e la sua inversa coincide con la sua trasposta, la soluzione
   # del sistema lineare Qy=b è ottenuta come y=Q^T*b
   y=Q.T@b
   xqr,flag=Usolve(R,y) #Soluzione del sistema triangolare superiore Rx=y
   err_rel_qr.append(np.linalg.norm(xqr-xesatta,2)/np.linalg.norm(xesatta,2))
   
   
plt.semilogy(range(48,59,2),err_rel,'ro-',range(48,59,2),err_rel_qr,'go-')
plt.legend(['Gauss','QR'])
plt.ylabel('Errore relativo sulla soluzione')
plt.show()



