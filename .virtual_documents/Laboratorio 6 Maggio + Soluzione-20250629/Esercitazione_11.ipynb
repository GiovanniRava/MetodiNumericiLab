


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D





def steepestdescent(A,b,x0,itmax,tol):
 
    n,m=A.shape
    if n!=m:
        print("Matrice non quadrata")
        return [],[]
    
    
   # inizializzare le variabili necessarie
    x = x0

     
    r = A@x-b
    p = -r
    it = 0
    nb=np.linalg.norm(b)
    errore=np.linalg.norm(r)/nb
    vec_sol=[]
    vec_sol.append(x.copy())
    vet_r=[]
    vet_r.append(errore)
     
# utilizzare il metodo del gradiente per trovare la soluzione
    while errore>= tol and it< itmax:
        it=it+1
        Ap=A@p
       
        alpha = -(r.T@p)/(p.T@Ap)
                
        x = x + alpha*p  #aggiornamento della soluzione nella direzione opposta a quella del gradiente: alpha mi dice dove fermarmi 
        #nella direzione del gradiente affinche F(xk+t p ) <F(xk)
        
         
        vec_sol.append(x.copy())
        r=r+alpha*Ap
        errore=np.linalg.norm(r)/nb
        vet_r.append(errore)
        p = -r #Direzione opposta alla direzione del gradiente
        
    iterates_array = np.vstack([arr.T for arr in vec_sol])
    return x,vet_r,iterates_array,it
    





def steepestdescent_CL(A,b,x0,itmax,X,Y,Z,f,tol):
#solo per matrici di dimensioni 2
    n,m=A.shape
    if n!=m:
        print("Matrice non quadrata")
        return [],[]
    
    
   # inizializzare le variabili necessarie
    x = x0
    
    plt.contour(X, Y, Z, levels=f(x,A,b).flatten())
    plt.plot(x[0],x[1],'r-o')
    r = A@x-b
    p = -r
    it = 0
    nb=np.linalg.norm(b)
    errore=np.linalg.norm(r)/nb
    vec_sol=[]
    vec_sol.append(x)
    vet_r=[]
    vet_r.append(errore)
     
# utilizzare il metodo del gradiente per trovare la soluzione
    while errore>= tol and it< itmax:
        it=it+1
        Ap=A@p
        
        alpha = -(r.T@p)/(p.T@Ap)
        x = x + alpha*p
       
        
        plt.contour(X, Y, Z, levels=f(x,A,b).flatten())
        plt.plot(x[0],x[1],'ro')
        vec_sol.append(x)
        r=r+alpha*Ap
        errore=np.linalg.norm(r)/nb
        vet_r.append(errore)
        p = -r 
        
    plt.show()
    iterates_array = np.vstack([arr.T for arr in vec_sol])
    return x,vet_r,iterates_array,it





def f(x,A,b):
    Ax = A@x
    xAx = x.T@Ax
    bx = b.T@x
    return 0.5 * xAx - bx



def conjugate_gradient(A,b,x0,itmax,tol):
    n,m=A.shape
    if n!=m:
        print("Matrice non quadrata")
        return [],[]
    
    
   # inizializzare le variabili necessarie
    x = x0
    
    r = A@x-b 
    p = -r
    it = 0
    nb=np.linalg.norm(b)
    errore=np.linalg.norm(r)/nb
    vec_sol=[]
    vec_sol.append(x0.copy())
    vet_r=[]
    vet_r.append(errore)
# utilizzare il metodo del gradiente coniugato per calcolare la soluzione
    while it<=itmax and errore>=tol: 
        it=it+1
        Ap=A.dot(p)
        alpha = -(r.T@p)/(p.T@Ap) 
        x = x0+alpha*p 
        vec_sol.append(x.copy())
        rtr_old=r.T@r
        r= r+alpha*Ap
        gamma=(r.T@r)/rtr_old
        errore=np.linalg.norm(r)/nb
        vet_r.append(errore)
        p = -r+gamma*p
   
    iterates_array = np.vstack([arr.T for arr in vec_sol])
    return x,vet_r,iterates_array,it


def conjugate_gradient_CL(A,b,x0,itmax,X,Y,Z,f,tol):
#solo per matrici di dimensioni 2
    n,m=A.shape
    if n!=m:
        print("Matrice non quadrata")
        return [],[]
    
    
   # inizializzare le variabili necessarie
    x = x0
    
    plt.contour(X, Y, Z, levels=f(x,A,b).flatten())
    plt.plot(x[0],x[1],'r-o')
    r = A@x-b
    p = -r
    it = 0
    nb=np.linalg.norm(b)
    errore=np.linalg.norm(r)/nb
    vec_sol=[]
    vec_sol.append(x)
    vet_r=[]
    vet_r.append(errore)
     
# utilizzare il metodo del gradiente per trovare la soluzione
    while errore>= tol and it< itmax:
        it=it+1
        Ap=A@p
        
        alpha = -(r.T@p)/(p.T@Ap)
        x = x + alpha*p
       
        
        plt.contour(X, Y, Z, levels=f(x,A,b).flatten())
        plt.plot(x[0],x[1],'ro')
        vec_sol.append(x)
        rtr_old=r.T@r
        r=r+alpha*Ap
        gamma=r.T@r/rtr_old
        errore=np.linalg.norm(r)/nb
        vet_r.append(errore)
        p = -r+gamma*p  #La nuova direzione appartiene al piano individuato da -r e p. gamma Ã¨ scelto in maniera tale che la nuova direzione
        #sia coniugata rispetto alla direzione precedente( che geometricamente significa che punti verso il centro)
   
        
    plt.show()
    iterates_array = np.vstack([arr.T for arr in vec_sol])
    return x,vet_r,iterates_array,it











A=np.array([[8,4],[4,3]]) 
b=np.array([[8],[10]])
x=np.linspace(-7,3,100)
y=np.linspace(-5,14,100)
 
X, Y = np.meshgrid(x, y)
Z = np.zeros(X.shape)
for i in range(len(y)):
    for j in range(len(x)):
        x_coor = X[i][j]
        y_coor = Y[i][j]
        Z[i][j] = f(np.array([[x_coor], [y_coor]]),A,b)
 
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plotta la superficie
ax.plot_surface(X, Y, Z, cmap=plt.cm.viridis)

# Mostra il grafico
plt.show() 



x0=np.zeros_like(b)
itmax=200
tol=1e-10
x_G_0,vet_r_G_0,vec_sol_G,itG_0=steepestdescent_CL(A,b,x0,itmax,X,Y,Z,f,tol)
print(itG_0)


print("Soluzione sistema ", x_G_0)
print("Numero di iterati ", itG_0)
# Visualizzazione grafica 3D
# Creazione della griglia per la superficie
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Plot della superficie

surf = ax.plot_surface(X, Y, Z, cmap='Blues', alpha=0.6, rstride=1, cstride=1, linewidth=0)

# Calcolo di z_iterates
z_iterates = np.array([f(vec_sol_G[i, :].reshape(2, 1), A, b) for i in range(vec_sol_G.shape[0])])

# Plot del percorso degli iterati
ax.plot(vec_sol_G[:, 0], vec_sol_G[:, 1], z_iterates.flatten(), color='red', linewidth=2, label='Percorso a zig-zag')
ax.scatter(vec_sol_G[:5, 0], vec_sol_G[:5, 1], z_iterates[:5].flatten(), color='red', s=80, label='Iterati')

# Punto iniziale e minimo

ax.scatter([x0[0, 0]], [x0[1, 0]], [f(x0, A, b)], color='blue', s=200, marker='o', label='Punto iniziale')
ax.scatter([x_G_0[0, 0]], [x_G_0[1, 0]], f(x_G_0,A,b), color='green', s=200, marker='*', label='Minimo trovato')

# Personalizzazione
ax.set_title(f'Metodo del Gradiente per f(x) = 0.5*x^T A x - b^T x')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
ax.legend()
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5, label='f(x, y)')
plt.show()
print("cond A",np.linalg.norm(A))








# Plot della superficie

surf = ax.plot_surface(X, Y, Z, cmap='Blues', alpha=0.6, rstride=1, cstride=1, linewidth=0)

# Calcolo di z_iterates
z_iterates = np.array([f(vec_sol_G[i, :].reshape(2, 1), A, b) for i in range(vec_sol_G.shape[0])])

# Plot del percorso degli iterati
ax.plot(vec_sol_G[:, 0], vec_sol_G[:, 1], z_iterates.flatten(), color='red', linewidth=2, label='Percorso a zig-zag')
ax.scatter(vec_sol_G[:5, 0], vec_sol_G[:5, 1], z_iterates[:5].flatten(), color='red', s=80, label='Iterati')

# Punto iniziale e minimo

ax.scatter([x0[0, 0]], [x0[1, 0]], [f(x0, A, b)], color='blue', s=200, marker='o', label='Punto iniziale')
ax.scatter([x_cg_0[0, 0]], [x_cg_0[1, 0]], f(x_cg_0,A,b), color='green', s=200, marker='*', label='Minimo trovato')

# Personalizzazione
ax.set_title(f'Metodo del Gradiente per f(x) = 0.5*x^T A x - b^T x')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
ax.legend()
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5, label='f(x, y)')
plt.show()
print("cond A",np.linalg.norm(A))


x0=np.zeros_like(b)
itmax=200
tol=1e-10
x_cg_0,vet_r_cg_0,vec_sol_cg,itcg_0=conjugate_gradient_CL(A,b,x0,itmax,X,Y,Z,f,tol)
print(itcg_0)














from scipy.linalg import hilbert
n=5
A2=hilbert(5)
b2=np.sum(A2,axis=1).reshape(n,1)
x2_0=np.zeros_like(b2)
itmax=3000
toll=1e-10





x_gr_2, vet_r_gr_2,vec_sol_gr2, itG_2=steepestdescent(A2,b2,x2_0,itmax,toll)
x_CG_2, vet_r_CG_2, vec_sol_CG2, itCG_2 = conjugate_gradient(A2,b2,x2_0, itmax, toll)
print("Numero di iterazioni del gradiente  ", itG_2)
print("Numero di iterazioni del gradiente coniugato ", itCG_2)
print("Condizionamento di Hilbert ",np.linalg.cond(A2))

plt.loglog(np.arange(itG_2+1), vet_r_gr_2,np.arange(itCG_2+1), vet_r_CG_2)






























